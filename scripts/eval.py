import argparse
from contextlib import nullcontext

import numpy as np
import torch
from omegaconf import OmegaConf
from pytorch_lightning import seed_everything
from tqdm.auto import tqdm
from transformers.models import opt

from ldm.models.diffusion.ddim import DDIMSampler
from ldm.models.diffusion.ddpm import LatentDiffusion
from ldm.modules.encoders.modules import CLIPReIDEmbedder
from ldm.util import instantiate_from_config


def load_model_from_config(config, ckpt, verbose=False):
    print(f"Loading model from {ckpt}")
    pl_sd = torch.load(ckpt, map_location="cpu")
    if "global_step" in pl_sd:
        print(f"Global Step: {pl_sd['global_step']}")
    sd = pl_sd["state_dict"]
    model = instantiate_from_config(config.model)
    m, u = model.load_state_dict(sd, strict=False)
    if len(m) > 0 and verbose:
        print("missing keys:")
        print(m)
    if len(u) > 0 and verbose:
        print("unexpected keys:")
        print(u)

    model.cuda()
    model.eval()
    return model


def evaluate(query_feature, query_label, gallery_features, gallery_labels):
    gallery_labels = np.array(gallery_labels)

    score = np.dot(gallery_features, query_feature)
    # index descending
    index = np.argsort(score)

    index = index[::-1]
    good_idx = np.argwhere(gallery_labels == query_label)

    junk_idx = np.argwhere(gallery_labels == -1)

    cmc_tmp = compute_mAP(index, good_idx, junk_idx)
    return cmc_tmp


def compute_mAP(index, good_index, junk_index):
    ap = 0
    cmc = torch.IntTensor(len(index)).zero_()
    if good_index.size == 0:  # if empty
        cmc[0] = -1
        return ap, cmc

    mask = np.in1d(index, junk_index, invert=True)
    index = index[mask]

    ngood = len(good_index)
    mask = np.in1d(index, good_index)
    rows_good = np.argwhere(mask == True)
    rows_good = rows_good.flatten()

    cmc[rows_good[0] :] = 1
    for i in range(ngood):
        d_recall = 1.0 / ngood
        precision = (i + 1) * 1.0 / (rows_good[i] + 1)
        if rows_good[i] != 0:
            old_precision = i * 1.0 / rows_good[i]
        else:
            old_precision = 1.0
        ap = ap + d_recall * (old_precision + precision) / 2.0

    return ap, cmc


def main():
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--ddim_steps",
        type=int,
        default=50,
        help="number of ddim sampling steps",
    )
    parser.add_argument(
        "--reid",
        action="store_true",
        help="uses the reid model",
    )
    parser.add_argument(
        "--ddim_eta",
        type=float,
        default=0.0,
        help="ddim eta (eta=0.0 corresponds to deterministic sampling",
    )
    parser.add_argument(
        "--size",
        type=int,
        default=8,
        help="latent feature size",
    )
    parser.add_argument(
        "--C",
        type=int,
        default=8,
        help="latent channels",
    )
    parser.add_argument(
        "--n_samples",
        type=int,
        default=1,
        help="how many samples to produce for each given prompt. A.k.a. batch size",
    )
    parser.add_argument(
        "--scale",
        type=float,
        default=5.0,
        help="unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=3407,
        help="the seed (for reproducible sampling)",
    )
    parser.add_argument(
        "--precision",
        type=str,
        help="evaluate at this precision",
        choices=["full", "autocast"],
        default="full",
    )
    parser.add_argument(
        "--extract_feature",
        "-e",
        action="store_true",
        help="if enabled, no image will be generated by first stage model, instead of feature",
    )
    parser.add_argument("--dataset", "-d", type=str, default="CUHK")

    opt = parser.parse_args()

    if opt.reid:
        print("Generating ReID images features...")
        opt.config = "configs/latent-diffusion/eval.yaml"
        opt.ckpt = "logs/2024-05-28T05-35-33_ldm/checkpoints/last.ckpt"

    assert opt.extract_feature
    assert opt.dataset is not None and opt.dataset in ["cuhk", "icfg", "rstp"]

    seed_everything(opt.seed)

    config = OmegaConf.load(f"{opt.config}")
    model = load_model_from_config(config, f"{opt.ckpt}")
    #
    # device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    # model = model.to(device)
    #
    sampler = DDIMSampler(model)

    data_module = None
    if opt.dataset == "cuhk":
        data_module = instantiate_from_config(config.cuhk)
    elif opt.dataset == "icfg":
        data_module = instantiate_from_config(config.icfg)
    elif opt.dataset == "rstp":
        data_module = instantiate_from_config(config.rstp)
    data_module.prepare_data()
    data_module.setup()

    # use test dataset to calculate mAP, rank1, rank5, rank10 and FID

    query_ids = []
    query_id_idx = []
    gallery_ids = []
    gallery_id_idx = []

    precision_scope = nullcontext

    with torch.no_grad():
        with precision_scope("cuda"):
            dataloader = data_module._test_dataloader()

            # len(dataloader) = number of batches of dataloader
            features = torch.zeros([len(dataloader.dataset), opt.C * opt.size**2])

            for batch_idx, batch in tqdm(enumerate(dataloader), total=len(dataloader)):
                # get captions of batch

                # split query set and gallery set
                base_idx = batch_idx * opt.n_samples

                for idx, i in enumerate(batch["id"]):
                    i = i.item()
                    if i not in query_ids:
                        query_ids.append(i)
                        query_id_idx.append((i, base_idx + idx))
                    else:
                        gallery_ids.append(i)
                        gallery_id_idx.append((i, base_idx + idx))

                if opt.scale != 0.0:
                    uc = model.get_learned_conditioning(
                        min(opt.n_samples, len(batch["id"])) * [""]
                    )
                else:
                    uc = None

                cond_1 = model.get_learned_conditioning(batch["caption_1"])
                cond_2 = model.get_learned_conditioning(batch["caption_2"])
                cond = torch.stack([cond_1, cond_2], dim=1)
                cond = torch.mean(cond, dim=1)

                shape = [opt.C, opt.size, opt.size]
                samples_ddim, _ = sampler.sample(
                    S=opt.ddim_steps,
                    conditioning=cond,
                    batch_size=min(opt.n_samples, len(batch["id"])),
                    shape=shape,
                    verbose=False,
                    unconditional_guidance_scale=opt.scale,
                    unconditional_conditioning=uc,
                    eta=opt.ddim_eta,
                    x_T=None,
                )

                ff = model.decode_first_stage(
                    samples_ddim, extract_feature=opt.extract_feature
                )
                ff = torch.flatten(ff, start_dim=1)
                ff = ff / ff.norm(dim=-1, keepdim=True)

                # z = model.get_input(batch, "image")
                # if isinstance(z, list):
                #     z = z[0]
                # ff = torch.flatten(z, start_dim=1)
                # ff = ff / ff.norm(dim=-1, keepdim=True)

                features[
                    base_idx : min(base_idx + opt.n_samples, len(dataloader.dataset)),
                    :,
                ] = ff

            query_idx = [t[-1] for t in query_id_idx]
            gallery_idx = [t[-1] for t in gallery_id_idx]
            query_features = torch.index_select(
                features, dim=0, index=torch.tensor(query_idx)
            )
            gallery_features = torch.index_select(
                features, dim=0, index=torch.tensor(gallery_idx)
            )

            query_features = query_features / query_features.norm(dim=1, keepdim=True)
            gallery_features = gallery_features / gallery_features.norm(
                dim=1, keepdim=True
            )

            query_features = query_features.numpy()
            gallery_features = gallery_features.numpy()

            cmc = torch.IntTensor(len(gallery_ids)).zero_()
            ap = 0.0
            for i in range(len(query_ids)):
                ap_tmp, cmc_tmp = evaluate(
                    query_features[i],
                    query_ids[i],
                    gallery_features,
                    gallery_ids,
                )
                if cmc_tmp[0] == -1:
                    continue
                cmc = cmc + cmc_tmp
                ap += ap_tmp
                # print(i, torch.argwhere(1 == cmc_tmp))

            cmc = cmc.float()
            cmc = cmc / len(query_ids)  # average CMC
            print(
                "Rank 1: %f Rank 5: %f Rank 10: %f Rank 100: %f mAP: %f"
                % (cmc[0], cmc[4], cmc[9], cmc[99], ap / len(query_ids))
            )
            # calculate indicators


if __name__ == "__main__":
    main()
